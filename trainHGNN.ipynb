{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import gc\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import partial, wraps\n",
    "from shadow.plot import *\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'DejaVu Sans'\n",
    "\n",
    "from graph_loader import graph_loader\n",
    "\n",
    "import sys\n",
    "MAINPATH = \"..\"  # nopep8\n",
    "sys.path.append(MAINPATH)  # nopep8\n",
    "from src import io_file\n",
    "from src.HGNN import *\n",
    "from src.models import *\n",
    "# from src import models\n",
    "# import importlib\n",
    "# importlib.reload(models)\n",
    "\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]\n",
    "\n",
    "def pprint(*args, namespace=globals()):\n",
    "    for arg in args:\n",
    "        print(f\"{namestr(arg, namespace)[0]}: {arg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = None\n",
    "rname=True\n",
    "withdata = None\n",
    "\n",
    "randfilename = datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\") + f\"_{datapoints}\"\n",
    "\n",
    "PSYS = f\"LJ3dab\"\n",
    "TAG = f\"HGNN\"\n",
    "out_dir = f\"../results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filename(name, tag=TAG):\n",
    "    rstring = randfilename if (rname and (tag != \"data\")) else (\n",
    "        \"0\" if (tag == \"data\") or (withdata == None) else f\"0_{withdata}\")\n",
    "    filename_prefix = f\"{out_dir}/{PSYS}-{tag}/{rstring}/\"\n",
    "    file = f\"{filename_prefix}/{name}\"\n",
    "    os.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "    filename = f\"{filename_prefix}/{name}\".replace(\"//\", \"/\")\n",
    "    print(\"===\", filename, \"===\")\n",
    "    return filename\n",
    "\n",
    "def OUT(f):\n",
    "    @wraps(f)\n",
    "    def func(file, *args, tag=TAG, **kwargs):\n",
    "        return f(_filename(file, tag=tag), *args, **kwargs)\n",
    "    return func\n",
    "\n",
    "loadfile = OUT(io_file.loadfile)\n",
    "savefile = OUT(io_file.savefile)\n",
    "save_ovito = OUT(io_file.save_ovito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arg_parser():\n",
    "    parser = argparse.ArgumentParser(description='StriderNET Torch arguments')\n",
    "    #Directories\n",
    "    parser.add_argument('--out_dir', default='./Output/',help='Output directory')\n",
    "    #Model args:\n",
    "    parser.add_argument('--node_emb_size', type=int, default=5,help='node embedding size')\n",
    "    parser.add_argument('--edge_emb_size', type=int, default=5,help='edge embedding size')\n",
    "    parser.add_argument('--hidden_emb_size', type=int, default=5,help='Hidden embedding size')\n",
    "    \n",
    "    parser.add_argument('--fa_layers', type=int, default=2,help='Initial node embedding MLP layers')\n",
    "    parser.add_argument('--fb_layers', type=int, default=2,help='Initial edge embedding MLP layers')\n",
    "    parser.add_argument('--fe_layers', type=int, default=2,help='Edge update MLP layers')\n",
    "    parser.add_argument('--fv_layers', type=int, default=2,help='node update MLP layers')\n",
    "    parser.add_argument('--MLP1_layers', type=int, default=2,help='MLP layers for node F from edge attribute')\n",
    "    parser.add_argument('--MLP1_F_out_dim', type=int, default=3,help='Force dimentions')\n",
    "    # parser.add_argument('--MLP2_layers', type=int, default=3,help='Displacement prediction MLP layers')\n",
    "    # parser.add_argument('--sigma', type=float, default=2.0,help='Displacement scaling factor')\n",
    "    parser.add_argument('--message_passing_steps', type=int, default=2,help='No. of message passing steps')\n",
    "    # parser.add_argument('--alpha',type=float,default=1e-6,help='Multivariate Gaussian standard deviation hyperparameter')\n",
    "    # parser.add_argument('--disp_cutoff', type=float, default=0.1,help='Cutoff for predicted displacement, for trianing stability')\n",
    "    \n",
    "    #Training Args:\n",
    "    parser.add_argument('--epochs', type=int, default=10000,help='No. of training epochs')\n",
    "    parser.add_argument('--lr', type=float, default=1e-4,help='Learning Rate')\n",
    "    parser.add_argument('--b_sz', type=int, default=20,help='Training batch size')\n",
    "    parser.add_argument('--seed', type=int, default=42,help='Seed value')\n",
    "    parser.add_argument('--dt', type=float, default=1e-5,help='dt value')\n",
    "    parser.add_argument('--kT', type=int, default=1,help='kT value')\n",
    "    parser.add_argument('--A', type=int, default=1,help='A value')\n",
    "    parser.add_argument('--B', type=int, default=125,help='B value')\n",
    "    # parser.add_argument('--train_len_ep', type=int, default=10,help='Length of optimization trajectory episode during training')\n",
    "    # parser.add_argument('--val_len_ep', type=int, default=20,help='Length of optimization trajectory episode during validation')\n",
    "    # parser.add_argument('--val_freq', type=int, default=10,help='Frequency of validation')\n",
    "    parser.add_argument('--cuda', action='store_true',help='use CUDA')\n",
    "    return parser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "\n",
    "def get_zdot_lambda(N, Dim, hamiltonian, drag=None, constraints=None, external_force=None):\n",
    "    dim = N * Dim\n",
    "    I = torch.eye(dim)\n",
    "    J = torch.zeros((2 * dim, 2 * dim))\n",
    "    J[:dim, dim:] = I\n",
    "    J[dim:, :dim] = -I\n",
    "    J2 = torch.zeros((2 * dim, 2 * dim))\n",
    "    J2[:dim, :dim] = I\n",
    "    J2[dim:, dim:] = I\n",
    "\n",
    "    def dH_dz(state_graph, params):\n",
    "        dH_dx = autograd.grad(hamiltonian, inputs='position', create_graph=True)(state_graph, params)\n",
    "        dH_dp = autograd.grad(hamiltonian, inputs='velocity', create_graph=True)(state_graph, params)\n",
    "        return torch.cat([dH_dx.flatten(), dH_dp.flatten()])\n",
    "\n",
    "    if drag is None:\n",
    "        def drag(state_graph, params):\n",
    "            return 0.0\n",
    "\n",
    "    def dD_dz(state_graph, params):\n",
    "        dD_dx = autograd.grad(drag, inputs='position', create_graph=True)(state_graph, params)\n",
    "        dD_dp = autograd.grad(drag, inputs='velocity', create_graph=True)(state_graph, params)\n",
    "        return torch.cat([dD_dx.flatten(), dD_dp.flatten()])\n",
    "\n",
    "    if external_force is None:\n",
    "        def external_force(state_graph, params):\n",
    "            return torch.zeros_like(state_graph[\"velocity\"])\n",
    "\n",
    "    if constraints is None:\n",
    "        def constraints(state_graph, params):\n",
    "            return torch.zeros((1, 2 * dim))\n",
    "\n",
    "    def fn_zdot(state_graph, params):\n",
    "        dH = dH_dz(state_graph, params)\n",
    "        dD = J2 @ dD_dz(state_graph, params)\n",
    "        dD = -J @ dD\n",
    "        F = torch.cat([torch.zeros(dim), external_force(state_graph, params).flatten()])\n",
    "        F = -J @ F\n",
    "        S = dH + J2 @ dD + F\n",
    "        A = constraints(state_graph, params).reshape(-1, 2 * dim)\n",
    "        Aᵀ = A.t()\n",
    "        INV = torch.pinverse(A @ J @ Aᵀ)\n",
    "        λ = -INV @ A @ J @ S\n",
    "        zdot = J @ (S + Aᵀ @ λ)\n",
    "        return zdot.reshape(2 * N, Dim)\n",
    "\n",
    "    def lambda_force(state_graph, params):\n",
    "        dH = dH_dz(state_graph, params)\n",
    "        dD = J2 @ dD_dz(state_graph, params)\n",
    "        dD = -J @ dD\n",
    "        F = torch.cat([torch.zeros(dim), external_force(state_graph, params).flatten()])\n",
    "        F = -J @ F\n",
    "        S = dH + J2 @ dD + F\n",
    "        A = constraints(state_graph, params).reshape(-1, 2 * dim)\n",
    "        Aᵀ = A.t()\n",
    "        INV = torch.pinverse(A @ J @ Aᵀ)\n",
    "        λ = -INV @ A @ J @ S\n",
    "        return (J @ Aᵀ @ λ).reshape(2 * N, Dim)\n",
    "\n",
    "    return fn_zdot, lambda_force\n",
    "\n",
    "\n",
    "def get_constraints(N, Dim, phi_, mass=None):\n",
    "    if mass is None:\n",
    "        mass = 1.0\n",
    "\n",
    "    def phi(x): return phi_(x.reshape(N, Dim))\n",
    "\n",
    "    def phidot(x, p):\n",
    "        Dphi = autograd.grad(phi, inputs='x', create_graph=True)(x.flatten())\n",
    "        pm = (p.flatten() / mass)\n",
    "        return Dphi @ pm\n",
    "\n",
    "    def psi(z):\n",
    "        x, p = torch.split(z, 2)\n",
    "        return torch.vstack([phi(x), phidot(x, p)])\n",
    "\n",
    "    def Dpsi(z):\n",
    "        return autograd.jacobian(psi)(z)\n",
    "\n",
    "    def fn(x, p, params):\n",
    "        z = torch.vstack([x, p])\n",
    "        return Dpsi(z)\n",
    "\n",
    "    return fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:28<00:00, 350.51it/s]\n"
     ]
    }
   ],
   "source": [
    "g_loader = graph_loader()\n",
    "Train_loader= g_loader.create_batched_States(Batch_size=20)\n",
    "Init_batch=next(iter(Train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=HGNN(in_edge_feats=Init_batch['edge_attr'].shape[1],\n",
    "                in_node_feats=Init_batch['x'].shape[1],\n",
    "                in_type_ohe_size=Init_batch['type'].shape[1],\n",
    "                node_emb_size=8,\n",
    "                edge_emb_size=8,\n",
    "                hidden_emb_size=5,\n",
    "                use_ke_model=True,\n",
    "                kemlp_layers=2,\n",
    "                fa_layers=2,\n",
    "                fb_layers=2,\n",
    "                fe_layers=2,\n",
    "                fv_layers=2,\n",
    "                MLP1_layers=2,\n",
    "                message_passing_steps=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zdot_model, lamda_force_model = get_zdot_lambda(\n",
    "    *Init_batch['position'].shape[-2:], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxcpuLGNN",
   "language": "python",
   "name": "jaxcpulgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
